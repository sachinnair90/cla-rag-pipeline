{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51294f66",
   "metadata": {},
   "source": [
    "# Chunking Experiments\n",
    "\n",
    "This notebook contains code for testing best chunking strategies, libraries and snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856f2f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "pdf_path = \"../../data/cao-pdfs/Cao Bouw en Infra 2025 - 2027.pdf\"\n",
    "\n",
    "TOKENIZER_ENCODING = \"cl100k_base\"  # For OpenAI models\n",
    "TOKENIZER_MAX_TOKENS = 8192  # Adjust based on your chosen model\n",
    "\n",
    "MAX_TOKENS = 8192  # Adjust based on your chosen model\n",
    "VECTOR_DIM = 1536  # Adjust based on your chosen embeddings model\n",
    "\n",
    "AZURE_SEARCH_ENDPOINT = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "AZURE_SEARCH_API_KEY = os.getenv(\"AZURE_SEARCH_API_KEY\")  # Ensure this is your Admin Key\n",
    "AZURE_SEARCH_INDEX_NAME = os.getenv(\"AZURE_SEARCH_INDEX_NAME\", \"cao-rag-sample\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-10-21\")\n",
    "AZURE_OPENAI_CHAT_MODEL_NAME = os.getenv(\n",
    "    \"AZURE_OPENAI_CHAT_MODEL_NAME\"\n",
    ")\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME = os.getenv(\n",
    "    \"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\", \"text-embedding-3-large\"\n",
    ")  # Using a deployed model named \"text-embeddings-3-large\n",
    "AZURE_OPENAI_EMBEDDING_MODEL_NAME = os.getenv(\n",
    "    \"AZURE_OPENAI_EMBEDDING_MODEL_NAME\", \"text-embedding-3-large\"\n",
    ")  # Using a deployed model named \"text-embeddings-3-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf0caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Using GPU: {torch.cuda.get_device_name(0)}: {torch.cuda.device_count()} GPU(s)\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "    print(f\"{torch.cpu.device_count()} CPU core(s) available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6473e824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "converter = DocumentConverter() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09783e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.datamodel.accelerator_options import AcceleratorDevice, AcceleratorOptions\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    PdfPipelineOptions,\n",
    ")\n",
    "from docling.datamodel.settings import settings\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "\n",
    "# Explicitly set the accelerator options\n",
    "accelerator_options = AcceleratorOptions(\n",
    "    num_threads=8, device=AcceleratorDevice.CUDA\n",
    ")\n",
    "\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "pipeline_options.accelerator_options = accelerator_options\n",
    "pipeline_options.do_ocr = False\n",
    "pipeline_options.do_table_structure = True\n",
    "pipeline_options.table_structure_options.do_cell_matching = True\n",
    "\n",
    "converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(\n",
    "            pipeline_options=pipeline_options,\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f256ba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the document\n",
    "conversion_result = converter.convert(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b126ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"outputs/02-chunking-experiments\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "doc_filename = conversion_result.input.file.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08184381",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Document has {len(conversion_result.document.pages)} pages and {len(conversion_result.document.tables)} tables.\")\n",
    "# print(f\"Document text content:\\n{conversion_result.document.export_to_markdown()}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6935d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export tables\n",
    "'''\n",
    "import pandas as pd\n",
    "\n",
    "for table_ix, table in enumerate(conversion_result.document.tables):\n",
    "    table_df: pd.DataFrame = table.export_to_dataframe(doc=conversion_result.document)\n",
    "    print(f\"## Table {table_ix}\")\n",
    "    print(table_df.to_markdown())\n",
    "\n",
    "    # Save the table as CSV\n",
    "    element_csv_filename = output_dir / f\"{doc_filename}-table-{table_ix + 1}.csv\"\n",
    "    table_df.to_csv(element_csv_filename)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2184fc24",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "We convert the Document into smaller chunks for embedding and indexing. The built-in HierarchicalChunker preserves structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab62dda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.chunking import HierarchicalChunker\n",
    "import tiktoken\n",
    "from docling_core.transforms.chunker.tokenizer.openai import OpenAITokenizer\n",
    "\n",
    "# Initialize tiktoken encoding for OpenAI embedding models\n",
    "encoding = tiktoken.get_encoding(TOKENIZER_ENCODING)\n",
    "\n",
    "# Create Docling's OpenAITokenizer wrapper\n",
    "tokenizer = OpenAITokenizer(tokenizer=encoding, max_tokens=TOKENIZER_MAX_TOKENS)\n",
    "\n",
    "# Instantiate HierarchicalChunker with tokenizer\n",
    "chunker = HierarchicalChunker(tokenizer=tokenizer, merge_peers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39953023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xxhash import xxh64\n",
    "doc_chunks = list(chunker.chunk(conversion_result.document))\n",
    "\n",
    "all_chunks = []\n",
    "for idx, c in enumerate(doc_chunks):\n",
    "    # Enrich chunks (example: add custom metadata or transform)\n",
    "    chunk_text = chunker.contextualize(c)\n",
    "\n",
    "    byte_data = chunk_text.encode('utf-8')\n",
    "    chunk_index = xxh64(byte_data).hexdigest()\n",
    "\n",
    "    all_chunks.append((chunk_index, chunk_text))\n",
    "\n",
    "print(f\"Total chunks from PDF: {len(all_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46433b1f",
   "metadata": {},
   "source": [
    "### Part 3: Create Azure AI Search Index and Push Chunk Embeddings\n",
    "Weâ€™ll define a vector index in Azure AI Search, then embed each chunk using Azure OpenAI and upload in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ec4de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIVectorizerParameters,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SearchIndex,\n",
    "    SimpleField,\n",
    "    VectorSearch,\n",
    "    VectorSearchProfile,\n",
    ")\n",
    "\n",
    "VECTOR_FIELD_NAME = \"content_vector\"\n",
    "CONTENT_FIELD_NAME = \"content\"\n",
    "\n",
    "index_client = SearchIndexClient(\n",
    "    AZURE_SEARCH_ENDPOINT, AzureKeyCredential(AZURE_SEARCH_API_KEY)\n",
    ")\n",
    "\n",
    "def create_search_index(index_name: str):\n",
    "    # Define fields\n",
    "    fields = [\n",
    "        SimpleField(name=\"chunk_id\", type=SearchFieldDataType.String, key=True),\n",
    "        SearchableField(name=CONTENT_FIELD_NAME, type=SearchFieldDataType.String),\n",
    "        SearchField(\n",
    "            name=VECTOR_FIELD_NAME,\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            searchable=True,\n",
    "            filterable=False,\n",
    "            sortable=False,\n",
    "            facetable=False,\n",
    "            vector_search_dimensions=VECTOR_DIM,\n",
    "            vector_search_profile_name=\"default\",\n",
    "        ),\n",
    "    ]\n",
    "    # Vector search config with an AzureOpenAIVectorizer\n",
    "    vector_search = VectorSearch(\n",
    "        algorithms=[HnswAlgorithmConfiguration(name=\"default\")],\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "                name=\"default\",\n",
    "                algorithm_configuration_name=\"default\",\n",
    "                vectorizer_name=\"default\",\n",
    "            )\n",
    "        ],\n",
    "        vectorizers=[\n",
    "            AzureOpenAIVectorizer(\n",
    "                vectorizer_name=\"default\",\n",
    "                parameters=AzureOpenAIVectorizerParameters(\n",
    "                    resource_url=AZURE_OPENAI_ENDPOINT,\n",
    "                    deployment_name=AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME,\n",
    "                    model_name=AZURE_OPENAI_EMBEDDING_MODEL_NAME,\n",
    "                    api_key=AZURE_OPENAI_API_KEY,\n",
    "                ),\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Create or update the index\n",
    "    new_index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search)\n",
    "    try:\n",
    "        index_client.delete_index(index_name)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    index_client.create_or_update_index(new_index)\n",
    "    print(f\"Index '{index_name}' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9e3882",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_search_index(AZURE_SEARCH_INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed36446",
   "metadata": {},
   "source": [
    "### Generate Embeddings and Upload to Azure AI Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd86b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "search_client = SearchClient(\n",
    "    AZURE_SEARCH_ENDPOINT, AZURE_SEARCH_INDEX_NAME, AzureKeyCredential(AZURE_SEARCH_API_KEY)\n",
    ")\n",
    "openai_client = AzureOpenAI(\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    ")\n",
    "\n",
    "\n",
    "def embed_text(text: str):\n",
    "    \"\"\"\n",
    "    Helper to generate embeddings with Azure OpenAI.\n",
    "    \"\"\"\n",
    "    response = openai_client.embeddings.create(\n",
    "        input=text, model=AZURE_OPENAI_EMBEDDING_MODEL_NAME, dimensions=VECTOR_DIM\n",
    "    )\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216718f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_docs = []\n",
    "for chunk_id, chunk_text in all_chunks:\n",
    "    embedding_vector = embed_text(chunk_text)\n",
    "    upload_docs.append(\n",
    "        {\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"content\": chunk_text,\n",
    "            \"content_vector\": embedding_vector,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a79cf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "for i in range(0, len(upload_docs), BATCH_SIZE):\n",
    "    subset = upload_docs[i : i + BATCH_SIZE]\n",
    "    resp = search_client.upload_documents(documents=subset)\n",
    "\n",
    "    all_succeeded = all(r.succeeded for r in resp)\n",
    "    print(\n",
    "        f\"Uploaded batch {i} -> {i + len(subset)}; all_succeeded: {all_succeeded}, \"\n",
    "        f\"first_doc_status_code: {resp[0].status_code}\"\n",
    "    )\n",
    "\n",
    "print(\"All chunks uploaded to Azure Search.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8affb3",
   "metadata": {},
   "source": [
    "### Part 4: Perform RAG over PDF\n",
    "Combine retrieval from Azure AI Search with Azure OpenAI Chat Completions (aka. grounding your LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4361f458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from azure.search.documents.models import VectorizableTextQuery\n",
    "\n",
    "def generate_chat_response(prompt: str, system_message: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Generates a single-turn chat response using Azure OpenAI Chat.\n",
    "    If you need multi-turn conversation or follow-up queries, you'll have to\n",
    "    maintain the messages list externally.\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "    if system_message:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    completion = openai_client.chat.completions.create(\n",
    "        model=AZURE_OPENAI_CHAT_MODEL_NAME, messages=messages, temperature=1\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "user_query = \"What are the Probationary period duration of 1-2 year employment contract as per the cao 2025-2027?\"\n",
    "user_embed = embed_text(user_query)\n",
    "\n",
    "vector_query = VectorizableTextQuery(\n",
    "    text=user_query,  # passing in text for a hybrid search\n",
    "    k_nearest_neighbors=5,\n",
    "    fields=VECTOR_FIELD_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd19d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = search_client.search(\n",
    "    search_text=user_query, vector_queries=[vector_query], select=[CONTENT_FIELD_NAME], top=10\n",
    ")\n",
    "\n",
    "retrieved_chunks = []\n",
    "for result in search_results:\n",
    "    snippet = result[CONTENT_FIELD_NAME]\n",
    "    retrieved_chunks.append(snippet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abed8669",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_str = \"\\n---\\n\".join(retrieved_chunks)\n",
    "rag_prompt = f\"\"\"\n",
    "You are an AI assistant helping answering questions about Dutch CAO.\n",
    "Use ONLY the text below to answer the user's question.\n",
    "If the answer isn't in the text, say you don't know.\n",
    "\n",
    "Context:\n",
    "{context_str}\n",
    "\n",
    "Question: {user_query}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "final_answer = generate_chat_response(rag_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ef3422",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRAG Prompt and Response:\")\n",
    "print(rag_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58581947",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFinal Answer:\")\n",
    "print(final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cao-processor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
